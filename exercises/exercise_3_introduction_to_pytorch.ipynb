{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a22e2274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4592ecdd",
   "metadata": {},
   "source": [
    "## Short introduction to PyTorch\n",
    "\n",
    "You can find the documentation to PyTorch here: https://pytorch.org/docs/stable/index.html\n",
    "\n",
    "For this exercise you will need the packages nn and optim of the torch package, but you will also need to transform your data into a Tensor. Here is a short introduction to everything you will need for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f56baca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Transforming a numpy array into a Tensor.\n",
    "arr = np.array([1, 2, 3])\n",
    "print(arr)\n",
    "arr_tensor = torch.Tensor(arr)\n",
    "print(arr_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd2980c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# You can also change the type of the tensor.\n",
    "print(arr_tensor.long())\n",
    "print(arr_tensor.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0299a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# To add a dimension at a specific place.\n",
    "print(arr_tensor.size())\n",
    "print(arr_tensor.unsqueeze(0).size())\n",
    "print(arr_tensor.unsqueeze(1).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac2de3c",
   "metadata": {},
   "source": [
    "## nn Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9cf950d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.4726, -0.0036]], grad_fn=<EmbeddingBackward0>)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The nn package includes many different layers and loss functions: https://pytorch.org/docs/stable/nn.html\n",
    "# For our case we will use the nn.Embedding layer, it will learn a representation of our users and items in\n",
    "# the same f-dimensional space, to allow us 'compare' these representations with each other,\n",
    "# estimatimating similarity between users and items directly.\n",
    "\n",
    "# For example, we have 3 differnt users.\n",
    "n_ids = 3\n",
    "\n",
    "# and want to learn a representation of size 2.\n",
    "factors = 2\n",
    "\n",
    "# We will create the following layer.\n",
    "emb = nn.Embedding(n_ids, factors)\n",
    "\n",
    "# which then can look up vectors (size 2) associated with a specific user_id from (0, 1, 2)\n",
    "emb(torch.Tensor([0]).long()) # embedding for user with id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76903bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with nn.Module you can create a callable model.\n",
    "\n",
    "class AddModule(nn.Module):\n",
    "    def forward(self, a, b):\n",
    "        return a + b\n",
    "    \n",
    "add_module = AddModule()\n",
    "add_module(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2061f439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights of Some Layer:\n",
      "Parameter containing:\n",
      "tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and with nn layers, you can create a model, which has learnable parameters, which can be then saved and loaded.\n",
    "\n",
    "class SomeModule(nn.Module):\n",
    "    def __init__(self, n_factors=10, n_classes=1):\n",
    "        super(SomeModule, self).__init__()\n",
    "        \n",
    "        self.some_layer = nn.Linear(n_factors, n_classes)\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "        # to initialize the weight of some layer with a constant value.\n",
    "        nn.init.constant_(self.some_layer.weight, 0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.act(self.some_layer(x))\n",
    "        \n",
    "\n",
    "# initialize module.\n",
    "some_module = SomeModule()\n",
    "\n",
    "# lets look at the weight.\n",
    "print(\"Weights of Some Layer:\")\n",
    "print(some_module.some_layer.weight)\n",
    "\n",
    "# saving model.\n",
    "torch.save(some_module.state_dict(), \"some_module.pt\")\n",
    "\n",
    "# loading model.\n",
    "some_module.load_state_dict(torch.load(\"some_module.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dce7bf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5170], grad_fn=<SigmoidBackward0>)\n",
      "Do something without gradients.\n",
      "tensor([0.5170])\n"
     ]
    }
   ],
   "source": [
    "# If you want to train a model, you need to first set it in train mode.\n",
    "some_module.train()\n",
    "\n",
    "print(some_module(torch.Tensor([0] * 10).float()))\n",
    "\n",
    "# If you want to evaluate a model, same thing.\n",
    "some_module.eval()\n",
    "\n",
    "# with torch.no_grad you can make sure, that the output does not have any gradients.\n",
    "with torch.no_grad():\n",
    "    print(\"Do something without gradients.\")\n",
    "    \n",
    "    print(some_module(torch.Tensor([0] * 10).float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d160bfe",
   "metadata": {},
   "source": [
    "## Optim Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe023bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.0001\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# There are two main optimizers that are commonly used.\n",
    "\n",
    "# The Adam optimizer.\n",
    "adam = optim.Adam(some_module.parameters(), lr=0.0001)\n",
    "\n",
    "print(adam)\n",
    "\n",
    "# The SGD Optimizer.\n",
    "sgd = optim.SGD(some_module.parameters(), lr=0.0001)\n",
    "\n",
    "print(sgd)\n",
    "\n",
    "# We are not going to cover how each of them works, for this we encourage you to inform yourself on this topic\n",
    "# or enroll in courses on the topic Deep Learning or Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85409137",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5416171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct labels:  tensor([0., 1., 0., 1.])\n",
      "Output of Model:  tensor([-1.,  3.,  5.,  0.])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "L1Loss()\n",
      "Loss:  tensor(2.2500)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MSELoss()\n",
      "Loss:  tensor(7.7500)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BCELoss()\n",
      "Activated output:  tensor([0.2689, 0.9526, 0.9933, 0.5000])\n",
      "Loss:  tensor(1.5154)\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# For training a model, we need also to define a function which should be optimized.\n",
    "# There are many different types of losses:\n",
    "\n",
    "bce_loss = nn.BCELoss()\n",
    "mse_loss = nn.MSELoss()\n",
    "l1_loss = nn.L1Loss()\n",
    "\n",
    "labels = torch.tensor(np.array([0, 1, 0, 1])).float()\n",
    "output_of_model = torch.tensor(np.array([-1, 3, 5, 0])).float()\n",
    "\n",
    "print(\"Correct labels: \", labels)\n",
    "print(\"Output of Model: \", output_of_model)\n",
    "\n",
    "print(\"-\" * 100)\n",
    "\n",
    "print(l1_loss)\n",
    "\n",
    "loss_output = l1_loss(output_of_model, labels)\n",
    "\n",
    "print(\"Loss: \", loss_output)\n",
    "\n",
    "print(\"-\" * 100)\n",
    "\n",
    "print(mse_loss)\n",
    "\n",
    "loss_output = mse_loss(output_of_model, labels)\n",
    "\n",
    "print(\"Loss: \", loss_output)\n",
    "\n",
    "print(\"-\" * 100)\n",
    "\n",
    "print(bce_loss)\n",
    "\n",
    "act_output = nn.Sigmoid()(output_of_model)\n",
    "print(\"Activated output: \", act_output)\n",
    "loss_output = bce_loss(act_output, labels)\n",
    "\n",
    "print(\"Loss: \", loss_output)\n",
    "\n",
    "print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7ea8f",
   "metadata": {},
   "source": [
    "## One Training Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a332ea8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this step:  0.4829416871070862\n"
     ]
    }
   ],
   "source": [
    "# So to train one episode, you need to do the following things..\n",
    "y = torch.Tensor([1]).float()\n",
    "\n",
    "# Set modus to train.\n",
    "some_module.train()\n",
    "\n",
    "# set optimizer to zero grad.\n",
    "sgd.zero_grad()\n",
    "\n",
    "# get output.\n",
    "x = torch.Tensor([0] * 10).float()\n",
    "y_hat = some_module(x)\n",
    "\n",
    "# caluclate loss.\n",
    "loss = l1_loss(y_hat, y)\n",
    "\n",
    "# perform backward on loss.\n",
    "loss.backward()\n",
    "\n",
    "# do step with optimizer.\n",
    "sgd.step()\n",
    "\n",
    "print(\"Loss in this step: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e912d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}