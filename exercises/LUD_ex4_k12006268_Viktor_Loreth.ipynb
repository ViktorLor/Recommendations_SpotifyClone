{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*UE Learning from User-generated Data, CP MMS, JKU Linz 2022*\n",
    "# Exercise 4: Evaluation\n",
    "\n",
    "In this exercise we'll have a closer look at two very different RecSys evaluation metrics and use them to compare the three algorithms we implemented so far to each other. Please consult the lecture slides and the presentation from UE Session 4 for a recap.\n",
    "\n",
    "The assignment submission deadline is 15.05.2022 23:59.\n",
    "\n",
    "Make sure to rename the notebook according to the convention:\n",
    "\n",
    "LUD22_ex03_k<font color='red'><Matr. Number\\></font>_<font color='red'><Surname-Name\\></font>.ipynb\n",
    "\n",
    "for example:\n",
    "\n",
    "LUD22_ex03_k000007_Bond_James.ipynb\n",
    "\n",
    "## Implementation\n",
    "In this exercise, as before, you are reqired to write a number of functions. Only implemented functions are graded. Insert your implementations into the templates provided. Please don't change the templates even if they are not pretty. Don't forget to test your implementation for correctness and efficiency.\n",
    "\n",
    "Please **only use libraries already imported in the notebook**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "import scipy.linalg as linalg\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random as rnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>TASK 1/2</font>: Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement DCG, nDCG and Average Artist entropy in the corresponding templates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCG Score\n",
    "Implement DCG following the input/output convention:\n",
    "#### Input:\n",
    "* prediction - (not an interaction matrix!) numpy array with recommendations. Row index corresponds to User_id, column index corresponds to the rank of the item mentioned in the sell. Every cell (i,j) contains **item id** recommended to the user (i) on the position (j) in the list. For example:\n",
    "\n",
    "The following predictions structure [[12, 7, 99], [0, 97, 6]] means that the user with id==1 (second row) got recommended item **0** on the top of the list, item **97** on the second place and item **6** on the third place.\n",
    "\n",
    "* test_interaction_matrix - (plain interaction matrix format as before!) interaction matrix constructed from interactions held out as a test set, rows - users, columns - items, cells - 0 or 1\n",
    "\n",
    "* topK - integer - top \"how many\" to consider for the evaluation. By default top 10 items are to be considered\n",
    "\n",
    "#### Output:\n",
    "* DCG score\n",
    "\n",
    "Don't forget, DCG is calculated for every user separately and then the average is returned.\n",
    "\n",
    "\n",
    "<font color='red'>**Attention!**</font> Use logarithm with base 2 for discounts! Remember that the top1 recommendation shouldn't get discounted! Users without interactions in the test set shouldn't contribute to the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dcg_score(predictions: np.ndarray, test_interaction_matrix: np.ndarray, topK=10) -> float:\n",
    "    \"\"\"\n",
    "    predictions - np.ndarray - predictions of the recommendation algorithm for each user.\n",
    "    test_interaction_matrix - np.ndarray - test interaction matrix for each user.\n",
    "\n",
    "    returns - float - mean dcg score over all user.\n",
    "    \"\"\"\n",
    "    score = None\n",
    "\n",
    "    # TODO: YOUR IMPLEMENTATION.\n",
    "\n",
    "    score = 0\n",
    "\n",
    "    # In case a interaction_row is completely empty\n",
    "    delete_list = []\n",
    "    for i, row in enumerate(test_interaction_matrix):\n",
    "        if np.all((row == 0)):\n",
    "            delete_list.append(i)\n",
    "\n",
    "    test_interaction_matrix = np.delete(test_interaction_matrix, delete_list, 0)\n",
    "    predictions = np.delete(predictions, delete_list, 0)\n",
    "\n",
    "    for user in range(predictions.shape[0]):\n",
    "        for sample in range(min(topK, predictions.shape[1])):\n",
    "            if test_interaction_matrix[user][sample]:\n",
    "                score += 1 / np.log2(2 + predictions[user][sample])\n",
    "\n",
    "    return score / predictions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([[0, 1, 2, 3], [3, 2, 1, 0]])\n",
    "test_interaction_matrix = np.array([[1, 0, 0, 0], [0, 0, 0, 1]])\n",
    "\n",
    "dcg_score = get_dcg_score(predictions, test_interaction_matrix, topK=4)\n",
    "\n",
    "assert np.isclose(dcg_score, 1), \"1 expected\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can DCG score be higher than 1?\n",
    "* Can the average DCG score be higher than 1?\n",
    "* Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nDCG Score\n",
    "\n",
    "Following the same parameter convention as for DCG implement nDCG metric.\n",
    "\n",
    "<font color='red'>**Attention!**</font> Remember that ideal DCG is calculated separetely for each user and depends on the number of tracks held out for them as a Test set! Use logarithm with base 2 for discounts! Remember that the top1 recommendation shouldn't get discounted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ndcg_score(predictions: np.ndarray, test_interaction_matrix: np.ndarray, topK=10) -> float:\n",
    "    \"\"\"\n",
    "    predictions - np.ndarray - predictions of the recommendation algorithm for each user.\n",
    "    test_interaction_matrix - np.ndarray - test interaction matrix for each user.\n",
    "    topK - int - topK recommendations should be evaluated.\n",
    "    \n",
    "    returns - average ndcg score over all users.\n",
    "    \"\"\"\n",
    "    score = None\n",
    "\n",
    "    # TODO: YOUR IMPLEMENTATION.\n",
    "\n",
    "    score = 0\n",
    "    # In case a interaction_row is completely empty\n",
    "    delete_list = []\n",
    "    for i, row in enumerate(test_interaction_matrix):\n",
    "        if np.all((row == 0)):\n",
    "            delete_list.append(i)\n",
    "\n",
    "    test_interaction_matrix = np.delete(test_interaction_matrix, delete_list, 0)\n",
    "    predictions = np.delete(predictions, delete_list, 0)\n",
    "\n",
    "    for user in range(predictions.shape[0]):\n",
    "        maximum = np.sum(test_interaction_matrix[user])\n",
    "\n",
    "        perfect_score = 0\n",
    "        local_score = 0\n",
    "        local_counter = 0  # To calculate the formula\n",
    "\n",
    "        for sample in range(min(topK, predictions.shape[1])):\n",
    "\n",
    "            if test_interaction_matrix[user, predictions[user, sample]]:\n",
    "                local_score += 1 / np.log2(2 + local_counter)\n",
    "            if sample < maximum:\n",
    "                perfect_score += 1 / np.log2(2 + local_counter)\n",
    "            local_counter += 1\n",
    "\n",
    "        score += local_score / perfect_score\n",
    "\n",
    "    return score / predictions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([[0, 1, 2, 3], [3, 2, 1, 0], [1, 2, 3, 0], [-1, -1, -1, -1]])\n",
    "test_interaction_matrix = np.array([[1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0]])\n",
    "\n",
    "ndcg_score = get_ndcg_score(predictions, test_interaction_matrix, topK=4)\n",
    "\n",
    "assert np.isclose(ndcg_score, 1), \"ndcg score is not correct.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can nDCG score be higher than 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Artist Entropy\n",
    "\n",
    "Calculate the metric of Diversity as the Average Artist entropy (see UE slides).\n",
    "#### Parameters:\n",
    "* predictions - as above for DCG and nDCG;\n",
    "* item_df - dataframe containing 'artist' and 'track' columns, index - track id (use corresponding data file)\n",
    "* topK - depth of the list to be evaluated, as before\n",
    "\n",
    "#### Result:\n",
    "Average Artist Entropy over users\n",
    "\n",
    "Recap, main points:\n",
    "* First calculate diversity for each user, then return the mean over users\n",
    "* For every user build distribution of recommended tracks over artists (within topK). This distribution cannot have more than topK bins! Dont forget to turn it into probability distribution dividing it by topK\n",
    "* Use the formula from the UE slides for the per-user entropy\n",
    "\n",
    "<font color='red'>**Attention!**</font> Use logarithm with base 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(predictions, item_df, perfect=False):\n",
    "    percentage = 1 / predictions.shape[0]\n",
    "    entropy = 0\n",
    "\n",
    "    if perfect:\n",
    "        for sample in predictions:\n",
    "            entropy -= percentage * np.log2(percentage)\n",
    "\n",
    "    else:\n",
    "        unique_actors = {item: 0 for item in np.unique(item_df.iloc[:, 0])}\n",
    "        for sample in predictions:\n",
    "            unique_actors[item_df.iloc[sample].values[0]] += 1\n",
    "\n",
    "        total = predictions.shape[0]\n",
    "\n",
    "        for value in unique_actors.values():\n",
    "            if value == 0:\n",
    "                continue\n",
    "            entropy -= value / total * np.log2(value / total)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def get_average_entropy_score(predictions: np.ndarray, item_df: pd.DataFrame, topK=10) -> float:\n",
    "    \"\"\"\n",
    "    predictions - np.ndarray - predictions of the recommendation algorithm for each user.\n",
    "    item_df - pd.DataFrame - information about each song with columns 'artist' and 'track'.\n",
    "    \n",
    "    returns - float - average entropy score of the predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    score = None\n",
    "\n",
    "    # TODO: YOUR IMPLEMENTATION.\n",
    "\n",
    "    # In case a prediction_row is completely empty\n",
    "    delete_list = []\n",
    "    for i, row in enumerate(predictions):\n",
    "        if np.all((row == -1)):\n",
    "            delete_list.append(i)\n",
    "\n",
    "    predictions = np.delete(predictions, delete_list, 0)\n",
    "\n",
    "    if topK > predictions.shape[1]:\n",
    "        raise IndexError\n",
    "    score = 0\n",
    "\n",
    "    for user in range(predictions.shape[0]):\n",
    "        perfect_score = shannon_entropy(predictions[user, 0:min(topK, predictions.shape[1])], item_df, perfect=True)\n",
    "\n",
    "        local_score = shannon_entropy(predictions[user, 0:min(topK, predictions.shape[1])], item_df)\n",
    "        score += local_score / perfect_score\n",
    "    return score / predictions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df = pd.DataFrame({'artist': ['A1', 'A1', 'A1', 'A1', 'A2', 'A3', 'A4']})\n",
    "predictions = np.array([[0, 1, 2, 3], [6, 5, 4, 3], [-1, -1, -1, -1]])\n",
    "avg_entr_score = get_average_entropy_score(predictions, item_df, topK=4)\n",
    "\n",
    "assert np.isclose(avg_entr_score, 0.5), \"average entropy score is not correct.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>TASK 2/2</font>: Evaluation\n",
    "Use provided rec.py (see imports below) to build a simple evaluation framework. It should be able to evaluate POP, ItemKNN (CF) and SVD.\n",
    "\n",
    "In the end for each algorithm you should be able to obtain results formatted as follows:\n",
    "\n",
    "```\n",
    "{'m': {'ndcg': <>, 'average_entropy': <>},\n",
    " 'f': {'ndcg': <>, 'average_entropy': <>},\n",
    " 'all': {'ndcg': <>, 'average_entropy': <>}}\n",
    "```\n",
    " \n",
    "Every metric calculated for three groupls of test users: only for female, only for male and for all users together.\n",
    "Every value should be an average, calculated over two different data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rec import svd_decompose, svd_recommend_to_list\n",
    "from rec import inter_matr_binary, split_interactions\n",
    "from rec import recTopK\n",
    "from rec import recTopKPop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data_3/'\n",
    "\n",
    "usr_path = path + 'sampled_1000_items_demo.txt'\n",
    "itm_path = path + 'sampled_1000_items_tracks.txt'\n",
    "inter_path = path + 'sampled_1000_items_inter.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A) Universal Launcher\n",
    "Receives a config as described, train and test interaction matrices.\n",
    "\n",
    "Returns a matrix of size (total number of users) x (topK). cells - recommended item ids, sorted according to the score. Fill the cells corresponding to users with no interactions in the test set with (-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_recommendations_for_algorithm(config, inter_matrix_train, inter_matrix_test) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    inter_matrix_train - np.ndarray - test interaction matrix over all users and items.\n",
    "    inter_matrix_test -  np.ndarray - train interaction matrix over all users and items.\n",
    "    config - dict - configuration of this evaluation following the structure:\n",
    "\n",
    "    config = {\n",
    "        \"algorithm\": str - one of ['SVD', 'CF', 'TopPop']\n",
    "        \"inter_train_file_paths\": str - inter train files over all splits,\n",
    "        \"inter_test_file_paths\": str - inter test files over all splits,\n",
    "        \"user_file_path\": str - usr_path,\n",
    "        \"item_file_path\": str - itm_path,\n",
    "        \"top_k\": int - number of recommendations to evaluate\n",
    "        \"n\": int - used for CF.\n",
    "        \"f\": int - length of hidden representations for SVD\n",
    "    }\n",
    "\n",
    "    returns - np.ndarray - list of recommendations for a specific algorithm in the shape (users, topK)\n",
    "                           filled with -1 for users NOT in the test set and filled with topK\n",
    "                           recommendations for users in the test set.\n",
    "    \"\"\"\n",
    "\n",
    "    rec = None\n",
    "\n",
    "    df_users = pd.read_csv(config['user_file_path'], sep='\\t', header=None, names=['location', 'age', 'gender', 'date'])\n",
    "    df_items = pd.read_csv(config['item_file_path'], sep='\\t', header=None, names=['artist', 'track'])\n",
    "\n",
    "    rec = np.full((len(df_users), config['top_k']), -1)\n",
    "\n",
    "    # TODO: YOUR IMPLEMENTATION.\n",
    "    algo = config['algorithm']\n",
    "    users = list(range(inter_matrix_test.shape[0]))\n",
    "    if algo == 'SVD':\n",
    "        U, V = svd_decompose(inter_matrix_train, config[\"f\"])\n",
    "\n",
    "        user_seen_items = [[] for i in range(inter_matrix_test.shape[0])]\n",
    "        for i, inter_row in enumerate(inter_matrix_train):\n",
    "            for j, item in enumerate(inter_row):\n",
    "                if item == 1:\n",
    "                    user_seen_items[i].append(j)\n",
    "\n",
    "        target_rec = svd_recommend_to_list(users, user_seen_items, U, V, config['top_k'])\n",
    "        for i, user in enumerate(users):\n",
    "            rec[user] = target_rec[i]\n",
    "\n",
    "    elif algo == 'CF':\n",
    "\n",
    "        for user in users:\n",
    "            rec[user], _ = recTopK(inter_matrix_train, user, config['top_k'], config['n'])\n",
    "\n",
    "\n",
    "    elif algo == 'TopPop':\n",
    "        for user in users:\n",
    "            rec[user] = recTopKPop(inter_matrix_train, user, config['top_k'])\n",
    "\n",
    "    else:\n",
    "        raise IndexError('wrong Algorithm Key')\n",
    "    return rec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Single set evaluation (already implemented)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def evaluate_predictions(predictions: np.ndarray, test_interaction_matrix: np.ndarray,\n",
    "                         item_df: pd.DataFrame, topK=10) -> dict:\n",
    "    \"\"\"\n",
    "    This function returns a dictinary with all scores of predictions.\n",
    "\n",
    "    predictions - np.ndarray - predictions of the algorithm over all users.\n",
    "    test_interaction_matrix - np.ndarray - test interaction matrix over all users and items.\n",
    "    item_df - pd.DataFrame - information about each item with columns: 'artist', 'track'\n",
    "    topK - int - topK prediction should be evaluated\n",
    "\n",
    "    returns - dict - calculated metric scores, contains keys \"ndcg\" and \"average_entropy\".\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {}\n",
    "    ndcg = get_ndcg_score(predictions, test_interaction_matrix, topK)\n",
    "    metrics['ndcg'] = ndcg\n",
    "\n",
    "    average_entropy = get_average_entropy_score(predictions, item_df, topK)\n",
    "    metrics['average_entropy'] = average_entropy\n",
    "\n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (B) User-group evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def evaluate_gender(predictions: np.ndarray, test_interaction_matrix: np.ndarray, user_df: pd.DataFrame,\n",
    "                    item_df: pd.DataFrame, num_users=500, topK=10) -> dict:\n",
    "    \"\"\"\n",
    "    This function will evaluate certain predictions for each gender individually and return a dictionary\n",
    "    following the structure:\n",
    "\n",
    "    {'gender_key': {'metric_key': metric_score}}\n",
    "\n",
    "    predictions - np.ndarray - predictions of the algorithm over all users.\n",
    "    test_interaction_matrix - np.ndarray - test interaction matrix over all users and items.\n",
    "    user_df - pd.DataFrame - information about each user with columns: location', 'age', 'gender', 'date'\n",
    "    item_df - pd.DataFrame - information about each item with columns: 'artist', 'track'\n",
    "    topK - int - topK prediction should be evaluated\n",
    "\n",
    "    returns - dict - calculated metric scores for each gender.\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # TODO: YOUR IMPLEMENTATION.\n",
    "    female_users = []\n",
    "    male_users = []\n",
    "\n",
    "    for index in range(user_df.shape[0]):\n",
    "        if user_df['gender'][index] == 'm':\n",
    "            male_users.append(index)\n",
    "        elif user_df['gender'][index] == 'f':\n",
    "            female_users.append(index)\n",
    "    female_prediction = predictions[female_users]\n",
    "    male_prediction = predictions[male_users]\n",
    "\n",
    "    ## adjust test_interaction_matrix to only predicted users;\n",
    "\n",
    "    metrics['m'] = evaluate_predictions(male_prediction, test_interaction_matrix[male_users], item_df, topK)\n",
    "\n",
    "    metrics['f'] = evaluate_predictions(female_prediction, test_interaction_matrix[female_users], item_df, topK)\n",
    "\n",
    "    metrics['all'] = evaluate_predictions(predictions, test_interaction_matrix, item_df, topK)\n",
    "\n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (C) Main evaluation function\n",
    "Interprets the config and returns evaluation report for a single algorithm:\n",
    "```\n",
    "{'m': {'ndcg': <>, 'average_entropy': <>},\n",
    " 'f': {'ndcg': <>, 'average_entropy': <>},\n",
    " 'all': {'ndcg': <>, 'average_entropy': <>}}\n",
    "```\n",
    "\n",
    "Please pay attention to how splits are created and saved into the corresponding variables (Split Data section below)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def evaluate_algorithm(config) -> dict:\n",
    "    \"\"\"\n",
    "    This function will evaluate a certain algorithm defined with the parameters in config by:\n",
    "    - going over all test and train files\n",
    "    - generating the recommendations for each data split\n",
    "    - calling evaluate gender to get the metrics for each recommendation for each data split\n",
    "\n",
    "    Then the average score for each gender and metric should be calculated over all data splits and\n",
    "    a dictionary should be returned following the structure:\n",
    "    {'gender_key': {'metric_key': avg_metric_score}}\n",
    "\n",
    "    config - dict - configuration of this evaluation following the structure:\n",
    "\n",
    "    config = {\n",
    "        \"algorithm\": str - one of ['SVD', 'CF', 'TopPop']\n",
    "        \"inter_train_file_paths\": str - array of inter train file paths (1 per split),\n",
    "        \"inter_test_file_paths\": str - array of inter test file paths (1 per split),\n",
    "        \"user_file_path\": str - usr_path,\n",
    "        \"item_file_path\": str - itm_path,\n",
    "        \"top_k\": int - number of recommendations to evaluate\n",
    "        \"n\": int - used for CF.\n",
    "        \"f\": int - length of hidden representations for SVD\n",
    "    }\n",
    "\n",
    "    returns - dict - average score of each metric for each gender over all data splits.\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {}\n",
    "    # TODO: YOUR IMPLEMENTATION.\n",
    "    filepaths_train = config['inter_train_file_paths']\n",
    "    filepaths_test = config['inter_test_file_paths']\n",
    "\n",
    "    songs = pd.read_csv(config['item_file_path'], delimiter='\\t', header=None, names=['Artist', 'Title'])\n",
    "    users = pd.read_csv(config['user_file_path'], delimiter='\\t', header=None,\n",
    "                        names=['location', 'age', 'gender', 'date'])\n",
    "\n",
    "    my_dict = {'m': {'ndcg': 0, 'average_entropy': 0}, 'f': {'ndcg': 0, 'average_entropy': 0},\n",
    "               'all': {'ndcg': 0, 'average_entropy': 0}}\n",
    "\n",
    "    for i in range(len(filepaths_test)):\n",
    "        my_train_matrix = inter_matr_binary(config['user_file_path'], config['item_file_path'], filepaths_train[i])\n",
    "        my_test_matrix = inter_matr_binary(config['user_file_path'], config['item_file_path'], filepaths_test[i])\n",
    "\n",
    "        rec = get_recommendations_for_algorithm(config, my_train_matrix, my_test_matrix)\n",
    "        my_dict2 = evaluate_gender(rec, my_test_matrix, users, songs)\n",
    "        for ele in my_dict:\n",
    "            for ele2 in my_dict[ele]:\n",
    "                my_dict[ele][ele2] += my_dict2[ele][ele2] / len(filepaths_test)\n",
    "\n",
    "    metrics = my_dict\n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Splitting Data (already implemented)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data_2:  6982\n",
      "Test data_2:  1213\n",
      "Train data_2:  6968\n",
      "Test data_2:  1227\n"
     ]
    }
   ],
   "source": [
    "train_inter_files = []\n",
    "test_inter_files = []\n",
    "\n",
    "num_splits = 2\n",
    "p_i = 0.3\n",
    "p_u = 0.5\n",
    "\n",
    "user_file_path = None\n",
    "inter_file_path = None\n",
    "\n",
    "user_file_path = usr_path\n",
    "inter_file_path = inter_path\n",
    "\n",
    "for i in range(num_splits):\n",
    "    split_interactions(inter_file=inter_file_path,\n",
    "                       user_file_path=user_file_path,\n",
    "                       p_u=p_u,\n",
    "                       p_i=p_i,\n",
    "                       res_test_file=\"inter_TEST_\" + str(i) + \".txt\",\n",
    "                       res_train_file=\"inter_TRAIN_\" + str(i) + \".txt\")\n",
    "\n",
    "    train_inter_files.append(\"inter_TRAIN_\" + str(i) + \".txt\")\n",
    "    test_inter_files.append(\"inter_TEST_\" + str(i) + \".txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "assert len(train_inter_files) == num_splits, \"Number of Train files do not match the requirement\"\n",
    "assert len(test_inter_files) == num_splits, \"Number of Test files do not match the requirement\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluating Every Algorithm\n",
    "Make sure everything works. Try running evaluation with the three configs below.\n",
    "We expect KNN to outperform other algorithms on our small data sample."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "{'m': {'ndcg': 0.09798054431234463, 'average_entropy': 1.0},\n 'f': {'ndcg': 0.10735625527582683, 'average_entropy': 1.0},\n 'all': {'ndcg': 0.0988300214801769, 'average_entropy': 1.0}}"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate TopPop\n",
    "config = {\n",
    "    \"algorithm\": \"TopPop\",  # ['SVD', 'CF', 'TopPop']\n",
    "    \"inter_train_file_paths\": train_inter_files,\n",
    "    \"inter_test_file_paths\": test_inter_files,\n",
    "    \"user_file_path\": usr_path,\n",
    "    \"item_file_path\": itm_path,\n",
    "    \"top_k\": 10,  # number of recommendations.\n",
    "    \"n\": 5,  # used for CF.\n",
    "    \"f\": 32,  # length of hidden representations\n",
    "}\n",
    "\n",
    "scores = evaluate_algorithm(config)\n",
    "scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "{'m': {'ndcg': 0.21700279518014298, 'average_entropy': 0.9993341271985312},\n 'f': {'ndcg': 0.1594155151195772, 'average_entropy': 1.0},\n 'all': {'ndcg': 0.21178515571156825, 'average_entropy': 0.9993949145815799}}"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate Item KNN (CF)\n",
    "config = {\n",
    "    \"algorithm\": \"CF\",  # ['SVD', 'CF', 'TopPop']\n",
    "    \"inter_train_file_paths\": train_inter_files,\n",
    "    \"inter_test_file_paths\": test_inter_files,\n",
    "    \"user_file_path\": usr_path,\n",
    "    \"item_file_path\": itm_path,\n",
    "    \"top_k\": 10,  # number of recommendations.\n",
    "    \"n\": 5,  # used for CF.\n",
    "    \"f\": 32,  # length of hidden representations\n",
    "}\n",
    "\n",
    "scores = evaluate_algorithm(config)\n",
    "scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "{'m': {'ndcg': 0.03523934615472972, 'average_entropy': 0.9998057870995716},\n 'f': {'ndcg': 0.024935414779959357, 'average_entropy': 1.0},\n 'all': {'ndcg': 0.03430576847983441, 'average_entropy': 0.9998235167529608}}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate SVD\n",
    "config = {\n",
    "    \"algorithm\": \"SVD\",  # ['SVD', 'CF', 'TopPop']\n",
    "    \"inter_train_file_paths\": train_inter_files,\n",
    "    \"inter_test_file_paths\": test_inter_files,\n",
    "    \"user_file_path\": usr_path,\n",
    "    \"item_file_path\": itm_path,\n",
    "    \"top_k\": 10,  # number of recommendations.\n",
    "    \"n\": 5,  # used for CF.\n",
    "    \"f\": 256,  # length of hidden representations\n",
    "}\n",
    "\n",
    "scores = evaluate_algorithm(config)\n",
    "scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_keys = ['m', 'f', 'all']\n",
    "m_keys = ['ndcg', 'average_entropy']\n",
    "\n",
    "assert all([k in g_keys for k in scores.keys()]), 'keys error'\n",
    "assert all([k in m_keys for k in scores['m'].keys()]), 'keys error'\n",
    "assert scores['all']['ndcg'] >= 0, \"metric score should be a number.\"\n",
    "assert scores['all']['average_entropy'] >= 0, \"metric score should be a number.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions and Potential Future Work\n",
    "* Do all algorithms treat Female and Male users similarly? Why?\n",
    "* How would you try improve performance of all three algorithms?\n",
    "* What other metrics would you consider to compare these recommender systems?\n",
    "* What other user groups would you investigate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}